---
title: "Get Started"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r development, include=FALSE}
library(testthat)
```

```{r development-1}
# Load already included functions
pkgload::load_all(export_all = FALSE)
```

```{r}
library(scales); library(kableExtra); library(printr)
library(tidyverse)
```

# Basic Usage

For the basic usage we need 2 Dataframes.

-   A **Source Dataframe**: The dataframe you want to match

-   A **Target Dataframe**: The dataframe you want to match your source dataframe against

As an example we will use two build-in example tables (i.e. table_source and table_target) which contain firm names and other identifying information such as country, city and address

But before we start, let's have a look on those tables and check if our dataframes are in the right shape. We can use the function **check_data()** for this.

## Check Data

**Target Dataframe**

```{r, echo=FALSE}
head(table_source)
```

**Target Dataframe**

```{r, echo=FALSE}
head(table_target)
```

------------------------------------------------------------------------

**Check Data**
There are a couple of important requirements before you are able to match using this package.

1.  Source and Target dataframe need a column called **id** and this id column **MUST BE UNIQUE**

2.  Source and Target dataframe need to have additional overlapping column names. (Those are the options for matching)

3.  The columns you want to match on should be unique in some combination. E.g. if you want to match n a name and a city at least the combination of those two variables must be unique.

For our example dataframes all these requirements are fulfilled.
```{r}
check_data(table_source, table_target)
```

## Standardize Data

But before we start matching it is always advisable to perform some string standardization on the variables you want to match on (i.e. ensure that we have only upper/lower case, maybe remove punctions, ...). You can use the build-in function **standardize_data()** for that purpose.

standardize_data() takes 3 arguments

1.  .data: The dataframe you want to standardize

2.  .cols: The columns you want to standradize

3.  .fun: A function to standardize strings. You can leave it NULL, then the build-in function **standardize_str()** is used.

```{r}
match_cols <- c("name", "city", "address")

tab_source <- standardize_data(table_source, match_cols)
tab_target <- standardize_data(table_target, match_cols)
```

As mentioned you could also use a custom function.

```{r}
.tmp <- standardize_data(table_source, match_cols, tolower)
head(.tmp)
```

After standardizing I'd recommend that you check your data again.

```{r}
check_data(tab_source, tab_target)
```

## Match Data

Now that we ensured that our data is reasonably standardized and in the right shape, we can start matching. We will use the function **match_data()**

**match_data()** takes several (also optional) arguments:

-   **.source:** Source dataframe

-   **.target:** Target Dataframe

-   **.cols:** The columns you want to match on

-   **.must_match:** Columns that MUST match. This argument is rather important for 2 reasons.

    -   It prevents the function to perform operations on unnecessary columns (Your data will be exactly matched upfront on those columns, so we don't need to perform similarity functions on this)

    -   It will ease the memory requirements for really large matching tables (more later)

-   **.max_match:** Maximum nuber of matches you want to retrieve

-   **.min_sim:** Minimum similarity of you chosen method.

-   **.method:** Metric used for matching (see stringdist-metrics {stringdist})

-   **.chunk:** You can chunk your Source dataframe, in case you run into memory issues

-   **.progress:** You can show a progress bar (might be useful if you data is really big)

For our data we choose the following parameters

```{r cache=TRUE}
tab_match <- match_data(
  .source = tab_source,
  .target = tab_target,
  .cols = match_cols,
  .must_match = "iso3",
  .max_match = 10,
  .min_sim = .2,
  .method = "osa",
  .chunk = 1,
  .progress = TRUE
)
```

Let's have a look at the resulting dataframe.

```{r}
head(tab_match)
```

## Score Matches

For each of your matching columns (i.e. name, city, address; we did no include iso3 for the approximate matching, if we had it would still have been excluded since the similarity is always 1) you get a similarity score defined by your method. The scores for the individual columns always have the prefix 'sim\_'. The resulting matches are in a n-m format, meaning one row of your source dataframe can be matched to multiple rows of your target dataframe and vice-versa.

So in the next step we will deduplicate the data. But before we can do this, we need a strategy to select the best matches. We will use the build-in function **score_data()** for this purpose.

Note: so far this function is rather crudely implemented, just taking a simple or weighted (squared) average of your data. This can and will be improved in the future.

First we use a simple average.

```{r}
tab_score1 <- scores_data(tab_match)
```

Second, let's try to assign some custom weights.

```{r}
tab_score2 <- scores_data(tab_match, c(.3, .1, .6))
```

## Choose Matches

Now we deduplicate the data, using the scores we just retrieved.

```{r}
tab_choose11 <- dedup_data(tab_score1, tab_source, tab_target, "score_mean")
tab_choose12 <- dedup_data(tab_score1, tab_source, tab_target, "score_square")

tab_choose21 <- dedup_data(tab_score2, tab_source, tab_target, "score_mean")
tab_choose22 <- dedup_data(tab_score2, tab_source, tab_target, "score_square")
```

Let's verify that the data is now in a 1-1 format

```{r}
verify <- function(.tab) {
  tab_ <- dplyr::filter(.tab, !is.na(id_s), !is.na(id_t))
  cat(
    paste0("Duplicated Source IDs: ", any(duplicated(tab_[["id_s"]]))),
    paste0("Duplicated Target IDs: ", any(duplicated(tab_[["id_t"]]))), 
    sep = "\n"
  )
}

verify(tab_choose11)
verify(tab_choose12)
verify(tab_choose21)
verify(tab_choose22)

```

## Compare Matches

Last let's evaluate how well the matching worked. You probably don't have a already matched sample yourself (I guess that's the whole purpose of this library), but I included an already matched sample as the build-in dataframe **table_matches.**

Let's write a quick function to evaluate the matches.

```{r}
get_accuracy <- function(.matches, .training, .name, .min = 0) {
  .matches %>%
    dplyr::select(id_s, id_t, score) %>%
    dplyr::filter(score >= .min | is.na(score)) %>%
    dplyr::full_join(dplyr::select(.training, id_s, id_t, match), by = c("id_s", "id_t")) %>%
    dplyr::summarise(
      p_correct = sum(match, na.rm = TRUE) / n(),
      p_incorrect = sum(is.na(match) & !is.na(id_t)) / n(),
      p_nomatch = sum(is.na(id_t)) / n(),
      total = p_correct + p_incorrect + p_nomatch,
      n = n()
    ) %>%
    dplyr::mutate(name = .name, .before = p_correct)
}

bind_rows(
  get_accuracy(tab_choose11, table_matches, "Unweighted Mean"),
  get_accuracy(tab_choose12, table_matches, "Unweighted Squared Mean"),
  get_accuracy(tab_choose21, table_matches, "Weighted Mean"),
  get_accuracy(tab_choose22, table_matches, "Weighted Squared Mean"),
) %>% arrange(desc(p_correct)) %>%
  mutate(
    across(matches("p_|total"), ~ scales::percent(., .01)),
    n = scales::comma(n)
    )

```

## Increase Threshold

With the Unweighted Squared Mean we get a matching accuracy of 89.69%, which is not bad I guess. But 5.45% of your data is incorrectly matched, and 5.36% of the data is not matched. Depending on your use case the unmatched data is less critical, but incorrectly matched data might harm your analysis.

Let's quickly check how this will develop if we set the threshold higher

```{r}
tab <- map_dfr(
  .x = set_names(seq(0, .95, .05), seq(0, .95, .05)),
  .f = ~ get_accuracy(tab_choose12, table_matches, "Unweighted Squared Mean", .x),
  .id = "min"
) %>%
  select(-name) %>%
  mutate(min = as.numeric(min)) 

mutate(tab, across(starts_with("p"), ~ scales::percent(., .01)))
  

```

```{r fig.width=7, echo=FALSE}
tab %>%
  pivot_longer(starts_with("p")) %>%
  ggplot(aes(min, value, color = name)) + 
  geom_line() + 
  geom_point() + 
  scale_y_continuous(labels = scales::percent) +
  geom_vline(xintercept = .5, linetype = "dashed") + 
  geom_vline(xintercept = .8, linetype = "dashed") +
  facet_wrap(~ name, scales = "free_y") +
  theme_minimal() + 
  theme(legend.position = "none")
```

We see, the higher the threshold, the better your matching.

## Account for Legal Forms

Especially for Company matching, we might have a lot of variety in how a legal form of a company is written (i.e. a public limited company can be written as plc, p.l.c, public limited company, ...). In the next step we try to standardize the legal forms.

```{r cache=TRUE}
match_cols_lf <- c("name_std", "city", "address")
tab_source_lf <- extract_legal_form(tab_source, "name")
tab_target_lf <- extract_legal_form(tab_target, "name")
```


```{r cache=TRUE}
tab_match_lf <- match_data(
  .source = tab_source_lf,
  .target = tab_target_lf,
  .cols = match_cols_lf,
  .must_match = "iso3",
  .max_match = 10,
  .min_sim = .2,
  .method = "osa",
  .chunk = 1,
  .progress = TRUE
)
```

```{r}
tab_score_lf <- scores_data(tab_match_lf)
tab_choose_lf <- dedup_data(tab_score_lf, tab_source_lf, tab_target_lf, "score_square")
```

```{r}
get_accuracy(tab_choose_lf, table_matches, "Unweighted Squared Mean") %>%
  mutate(
    across(matches("p_|total"), ~ scales::percent(., .01)),
    n = scales::comma(n)
    )
```

Standardizing the legal forms gave us a small improvement. For our chosen method we are able to increse the number of correct matches from **89.69% to 90.03%** (Not a huge increse, but the data test data is pretty standardized per default, you'll see bigger improvements for more unstandardized data)

## Simple Join
Last, we should probably compare the matching accuracies to the case where we just simply join the two datasets on the name and iso3 column.

**No Legal Form Standardization**
```{r}
tab_match_join <- tab_source %>%
  left_join(tab_target, by = c("name", "iso3"), suffix = c("_s", "_t")) %>%
  mutate(score = NA_real_) # Small Hack to use the same function

get_accuracy(tab_match_join, table_matches, "Unweighted Squared Mean") %>%
  mutate(
    across(matches("p_|total"), ~ scales::percent(., .01)),
    n = scales::comma(n)
    )
```

**With Legal Form Standardization**
```{r}
tab_match_join_lf <- tab_source_lf %>%
  left_join(tab_target_lf, by = c("name_std", "iso3"), suffix = c("_s", "_t")) %>%
  mutate(score = NA_real_) # Small Hack to use the same function

get_accuracy(tab_match_join_lf, table_matches, "Unweighted Squared Mean") %>%
  mutate(
    across(matches("p_|total"), ~ scales::percent(., .01)),
    n = scales::comma(n)
    )

```

<!-- # Memory Considerations -->

<!-- ```{r include = FALSE} -->
<!-- make_facke_data <- function(.n, .cols) { -->
<!--   s_ <- t_ <- tibble::tibble(id = as.character(seq_len(.n))) -->
<!--   for (i in .cols) { -->
<!--     s_[[i]] <- purrr::map_chr(paste(seq_len(.n), 1), ~ digest::digest(.x, "xxhash32")) -->
<!--     t_[[i]] <- purrr::map_chr(paste(seq_len(.n), 2), ~ digest::digest(.x, "xxhash32")) -->
<!--   } -->
<!--   return(list(s = s_, t = t_)) -->
<!-- } -->
<!-- ``` -->

<!-- ```{r include = FALSE, cache = TRUE} -->
<!-- # .dir_cache <- file.path(here::here(), "cache") -->
<!-- options(scipen = 999) -->
<!-- cols <- c("name", "city") -->

<!-- n00010 <- make_facke_data(10, cols) -->
<!-- n00100 <- make_facke_data(100, cols) -->
<!-- n01000 <- make_facke_data(1000, cols) -->
<!-- n10000 <- make_facke_data(10000, cols) -->

<!-- .tmp <- bench::mark( -->
<!--   n00010 = match_data(n00010$s, n00010$t, cols, .progress = FALSE), -->
<!--   n00100 = match_data(n00100$s, n00100$t, cols, .progress = FALSE), -->
<!--   n01000 = match_data(n01000$s, n01000$t, cols, .progress = FALSE), -->
<!--   n10000 = match_data(n10000$s, n10000$t, cols, .progress = FALSE), -->
<!--   check = FALSE -->
<!-- ) %>% dplyr::select(expression:mem_alloc) -->
<!-- rm(n00010, n00100, n01000, n10000) -->
<!-- ``` -->

<!-- ```{r include = FALSE} -->
<!-- benchmark <- .tmp %>% -->
<!--   dplyr::rename(size_mat = expression, mem_calc = mem_alloc) %>% -->
<!--   dplyr::mutate( -->
<!--     dplyr::across(c(min, median), as.numeric), -->
<!--     tmp = as.integer(gsub("n", "", size_mat)), -->
<!--     dim_mat = paste(comma(tmp), "x", comma(tmp)), -->
<!--     size_mat = tmp ^ 2, -->
<!--     mem_calc = as.numeric(mem_calc) / 1e6, -->
<!--     mem_mat = size_mat * 8 / 1e6, -->
<!--     `calc/mat` = as.numeric(mem_calc / mem_mat) -->
<!--   ) %>% -->
<!--   dplyr::mutate( -->
<!--     fac_size = size_mat / dplyr::lag(size_mat), -->
<!--     fac_mem = mem_calc / dplyr::lag(mem_calc), -->
<!--     fac_time = median / dplyr::lag(median), -->
<!--     # dplyr::across(!dplyr::matches("_mat$"), ~ round(., 2)) -->
<!--     ) %>% -->
<!--   dplyr::select(dim_mat, dplyr::everything(), -min, -`itr/sec`, -tmp) -->
<!-- ``` -->

<!-- ```{r echo = FALSE} -->
<!-- benchmark %>% -->
<!--   kableExtra::kbl(align = "c", format.args = list(big.mark = ",")) %>% -->
<!--   kableExtra::kable_paper() -->
<!-- ``` -->

```{r development-inflate, eval=FALSE}
# Run but keep eval=FALSE to avoid infinite loop
# Execute in the console directly
fusen::inflate(flat_file = "dev/flat_vignette.Rmd", vignette_name = "Get-Started")
```
