---
title: "Get-Started"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Get-Started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(Rmatch)
```

<!-- This vignette is generated by fusen: do not edit by hand -->

```{r include = FALSE}
library(scales); library(tidyverse)
```

# Memory Considerations

```{r include = FALSE}
make_facke_data <- function(.n, .cols) {
  s_ <- t_ <- tibble::tibble(id = as.character(seq_len(.n)))
  for (i in .cols) {
    s_[[i]] <- purrr::map_chr(paste(seq_len(.n), 1), ~ digest::digest(.x, "xxhash32"))
    t_[[i]] <- purrr::map_chr(paste(seq_len(.n), 2), ~ digest::digest(.x, "xxhash32"))
  }
  return(list(s = s_, t = t_))
}
```

```{r include = FALSE, cache = TRUE}
# .dir_cache <- file.path(here::here(), "cache")
options(scipen = 999)
cols <- c("name", "city")

n00010 <- make_facke_data(10, cols)
n00100 <- make_facke_data(100, cols)
n01000 <- make_facke_data(1000, cols)
n10000 <- make_facke_data(10000, cols)

.tmp <- RFgen::cache(
  .expr = bench::mark(
    n00010 = match_data(n00010$s, n00010$t, cols, .progress = FALSE),
    n00100 = match_data(n00100$s, n00100$t, cols, .progress = FALSE),
    n01000 = match_data(n01000$s, n01000$t, cols, .progress = FALSE),
    n10000 = match_data(n10000$s, n10000$t, cols, .progress = FALSE),
    check = FALSE
  ) %>% dplyr::select(expression:mem_alloc),
  .dir = "E:/R/R_projects/Rmatch/cache",
  .name = "memory_benchmark"
)
rm(n00010, n00100, n01000, n10000)
```

```{r include = FALSE, cache = TRUE}
benchmark <- .tmp %>%
  dplyr::rename(size_mat = expression, mem_calc = mem_alloc) %>%
  dplyr::mutate(
    dplyr::across(c(min, median), as.numeric),
    tmp = as.integer(gsub("n", "", size_mat)),
    dim_mat = paste(comma(tmp), "x", comma(tmp)),
    size_mat = tmp ^ 2,
    mem_calc = as.numeric(mem_calc) / 1e6,
    mem_mat = size_mat * 8 / 1e6,
    `calc/mat` = as.numeric(mem_calc / mem_mat)
  ) %>%
  dplyr::mutate(
    fac_size = size_mat / dplyr::lag(size_mat),
    fac_mem = mem_calc / dplyr::lag(mem_calc),
    fac_time = median / dplyr::lag(median),
    # dplyr::across(!dplyr::matches("_mat$"), ~ round(., 2))
    ) %>%
  dplyr::select(dim_mat, dplyr::everything(), -min, -`itr/sec`, -tmp)
```

```{r echo = FALSE}
benchmark %>%
  kableExtra::kbl(align = "c", format.args = list(big.mark = ",")) %>%
  kableExtra::kable_paper()
```

# Example Usage

For the basic usage we need 2 Dataframes.

-   A **Source Dataframe**: The dataframe you want to match

-   A **Target Dataframe**: The dataframe you want to match your source dataframe against

As an example we will use two build-in example tables (i.e. table_source and table_target) which contain firm names and other identifying information such as country, city and address

But before we start, let's have a look on those tables and check if our dataframes are in the right shape. We can use the function **check_data()** for this.


```{r}
table_source
table_target
check_data(table_source, table_target)
```

There are a couple of important requirements before you are able to match using this package.

1.  Source and Target dataframe need a column called **id** and this id column **MUST be unique**

2.  Source and Target dataframe need to have additional overlapping column names. (Those are the options for matching)

3.  The columns you want to match on should be unique in some combination. E.g. if you want to match n a name and a city at least the combination of those two variables must be unique.

For our example dataframes all these requirements are fulfilled.

But before we start matching it is always advisable to perform some string standardization on the variables you want to match on (i.e. ensure that we have only upper/lower case, maybe remove punctions, ...). You can use the build-in function **standardize_data()** for that purpose.

standardize_data() takes 3 arguments

1.  .data: The dataframe you want to standardize

2.  .cols: The columns you want to standradize

3.  .fun: A function to standardize strings. You can leave it NULL, then the build-in function **standardize_str()** is used.


```{r}
match_cols <- c("name", "city", "address")

tab_source <- standardize_data(table_source, match_cols)
tab_target <- standardize_data(table_target, match_cols)
```

As mentioned you could also use a custom function.


```{r}
standardize_data(table_source, match_cols, tolower)
```

After standardizing I'd recommend that you check your data again.


```{r}
check_data(tab_source, tab_target)
```

Now that we ensured that our data is reasonably standardized and in the right shape, we can start matching. We will use the function **match_data()**

**match_data()** takes several (also optional) arguments:

-   **.source:** Source dataframe

-   **.target:** Target Dataframe

-   **.cols:** The columns you want to match on

-   **.must_match:** Columns that MUST match. This argument is rather important for 2 reasons.

    -   It prevents the function to perform operations on unnecessary columns (Your data will be exactly matched upfront on those columns, so we don't need to perform similarity functions on this)

    -   It will ease the memory requirements for really large matching tables (more later)

-   **.max_match:** Maximum nuber of matches you want to retrieve

-   **.min_sim:** Minimum similarity of you chosen method.

-   **.method:** Metric used for matching (see stringdist-metrics {stringdist})

-   **.chunk:** You can chunk your Source dataframe, in case you run into memory issues

-   **.progress:** You can show a progress bar (might be useful if you data is really big)

For our data we choose the following parameters


```{r}
tab_match <- match_data(
  .source = tab_source,
  .target = tab_target,
  .cols = match_cols,
  .must_match = "iso3",
  .max_match = 10,
  .min_sim = .2,
  .method = "osa",
  .chunk = 1,
  .progress = TRUE
)
```

Let's have a look at the resulting dataframe.


```{r}
tab_match
```

For each of your matching columns (i.e. name, city, address; we did no include iso3 for the approximate matching, if we had it would still have been excluded since the similarity is always 1) you get a similarity score defined by your method. The scores for the individual columns always have the prefix 'sim\_'. The resulting matches are in a n-m format, meaning one row of your source dataframe can be matched to multiple rows of your target dataframe and vice-versa.

So in the next step we will deduplicate the data. But before we can do this, we need a strategy to select the best matches. We will use the build-in function **score_data()** for this purpose.

Note: so far this function is rather crudely implemented, just taking a simple or weighted (squared) average of your data. This can and will be improved in the future.

First we use a simple average.


```{r}
tab_score1 <- scores_data(tab_match)
```

Second, let's try to assign some custom weights.


```{r}
tab_score2 <- scores_data(tab_match, c(.3, .1, .6))
```

Now we deduplicate the data, using the scores we just retrieved.


```{r}
tab_choose11 <- dedup_data(tab_score1, tab_source, tab_target, "score_mean")
tab_choose12 <- dedup_data(tab_score1, tab_source, tab_target, "score_square")

tab_choose21 <- dedup_data(tab_score2, tab_source, tab_target, "score_mean")
tab_choose22 <- dedup_data(tab_score2, tab_source, tab_target, "score_square")
```

Let's verify that the data is now in a 1-1 format


```{r}
verify <- function(.tab) {
  tab_ <- dplyr::filter(.tab, !is.na(id_s), !is.na(id_t))
  cat(
    paste0("Duplicated Source IDs: ", any(duplicated(tab_[["id_s"]]))),
    paste0("Duplicated Target IDs: ", any(duplicated(tab_[["id_t"]]))), 
    sep = "\n"
  )
}

verify(tab_choose11)
verify(tab_choose12)
verify(tab_choose21)
verify(tab_choose22)

```

Last let's evaluate how well the matching worked. You probably don't have a already matched sample yourself (I guess that's the whole purpose of this library), but I included an already matched sample as the build-in dataframe **table_matches.**

Let's write a quick function to evaluate the matches.


```{r}
.matches <- tab_choose11
.training <- table_matches
.name = "test"
get_accuracy <- function(.matches, .training, .name, .min = 0) {
  .matches %>%
    dplyr::select(id_s, id_t, score) %>%
    dplyr::filter(score >= .min | is.na(score)) %>%
    dplyr::full_join(dplyr::select(.training, id_s, id_t, match), by = c("id_s", "id_t")) %>%
    dplyr::summarise(
      p_correct = sum(match, na.rm = TRUE) / n(),
      p_incorrect = sum(is.na(match) & !is.na(id_t)) / n(),
      p_nomatch = sum(is.na(id_t)) / n(),
      total = p_correct + p_incorrect + p_nomatch,
      n = n()
    ) %>%
    dplyr::mutate(name = .name, .before = p_correct)
}

bind_rows(
  get_accuracy(tab_choose11, table_matches, "Unweighted Mean"),
  get_accuracy(tab_choose12, table_matches, "Unweighted Squared Mean"),
  get_accuracy(tab_choose21, table_matches, "Weighted Mean"),
  get_accuracy(tab_choose22, table_matches, "Weighted Squared Mean"),
) %>% arrange(desc(p_correct)) %>%
  mutate(across(starts_with("p"), ~ scales::percent(., .01)))

```

With the Unweighted Squared Mean we get a matching accuracy of 88.62%, which is not bad I guess. But 6.02% of your data is incorrectly matched, and 5.36% of the data is not matched. Depending on your use case the unmatched data is less critical, but incorrectly matched data might harm your analysis.

Let's quickly check how this will develop if we set the threshold higher


```{r}
tab <- map_dfr(
  .x = set_names(seq(0, .95, .05), seq(0, .95, .05)),
  .f = ~ get_accuracy(tab_choose12, table_matches, "Unweighted Squared Mean", .x),
  .id = "min"
) %>%
  select(-name) %>%
  mutate(min = as.numeric(min)) 

mutate(tab, across(starts_with("p"), ~ scales::percent(., .01)))
  

```

```{r}
tab %>%
  pivot_longer(starts_with("p")) %>%
  ggplot(aes(min, value, color = name)) + 
  geom_line() + 
  geom_point() + 
  scale_y_continuous(labels = scales::percent) +
  geom_vline(xintercept = .5, linetype = "dashed") + 
  geom_vline(xintercept = .8, linetype = "dashed") +
  facet_wrap(~ name, scales = "free_y") +
  theme_minimal()
```

We see, the hiher the threshold, the better




